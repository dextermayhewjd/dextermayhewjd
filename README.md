# Hi there, I'm Dexter Ding (Jiahua DIng) ðŸ‘‹ 

AI Engineer focused on Large Language Model (LLM) training, inference optimization.
Currently developing expertise in distributed fine-tuning (LoRA/DPO), GPU kernel optimization (CUDA/Triton).
Combines solid ML theory, PyTorch engineering, and system-level optimization to build scalable, high-efficiency AI solutions.

"Building a complete stack understanding from model math to GPU execution pipeline,
and from fine-tuning to intelligent agent orchestration."


<img align="right">
  <img width="250" src="https://media.tenor.com/wkjOrQp6eJgAAAAd/i-am-iron-man-iron-man.gif">
</img>

### Languages and Tools:

| Domain                     | Skills / Tools                                                               |
| -------------------------- | ----------------------------------------------------------------------       |
| **LLM Training**           | PyTorch, HuggingFace Transformers, PEFT (LoRA), DeepSpeed                    |
| **Inference Optimization** | vLLM, TensorRT-LLM, AutoGPTQ, CUDA, Nsight Systems                           |
| **GPU & Systems**          | CUDA C++, PTX profiling, memory hierarchy tuning, Cutlass , cuBLAS, cuDNN    |
| **Efficient Computing**    | Pruning, Quantization, Knowledge Distillation, Distributed Training          |                                                          |
| **Math Foundations**       | Linear Algebra, Probability, Information Theory (KL, CE), Optimization       |

<br />
<br />
  <summary>:zap: GitHub Stats</summary>


